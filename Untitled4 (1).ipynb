{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b367c2-6e57-4a96-a0da-4d2ba9843535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: intel-extension-for-pytorch==2.2 in ./.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from intel-extension-for-pytorch==2.2) (2.2.3)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from intel-extension-for-pytorch==2.2) (21.3)\n",
      "Requirement already satisfied: psutil in ./.local/lib/python3.10/site-packages (from intel-extension-for-pytorch==2.2) (7.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.35.2 in ./.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.35.2) (2.25.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers==4.35.2) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.29.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.35.2) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (2.2.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers==4.35.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.2.0 in ./.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (1.13.3)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (3.17.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (2025.3.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.2.0) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch==2.2.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install intel-extension-for-pytorch==2.2\n",
    "!pip install transformers==4.35.2\n",
    "!pip install torch==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757f1974-6c45-4842-a496-95375592938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/srv/jupyter/python-venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/srv/jupyter/python-venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/srv/jupyter/python-venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install intel-extension-for-pytorch==2.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install transformers==4.35.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install torch==2.2.0 --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2dd29c5-a912-46bc-a851-8c11f732f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82106b48-ab3a-4905-afed-6ce00f6ef5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35e4578-1bd5-4ff0-b23d-7465602f8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c9f89148b7474b89443e8557a10b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Model='Intel/neural-chat-7b-v3-3'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(Model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0860fd25-0352-4bba-956e-562b2e69e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/quantization/_quantize.py:97: UserWarning: BatchNorm folding failed during the prepare process.\n",
      "  warnings.warn(\"BatchNorm folding failed during the prepare process.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex.llm.optimize is doing the weight only quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:2017: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/transformers/modeling_attn_mask_utils.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:2043: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  else torch.tensor(expanded_attn_mask) + torch.tensor(causal_4d_mask)\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:2043: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  else torch.tensor(expanded_attn_mask) + torch.tensor(causal_4d_mask)\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/reference/fusions/mha_fusion.py:41: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len is not None and seq_len > self.max_seq_len_cached:\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py:86: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  seq_info = torch.tensor(\n",
      "/home/u348972f67d39e06d5ddfe43a999e9a2/.local/lib/python3.11/site-packages/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq_info = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex.llm.optimize has set the optimized or quantization model for model.generate()\n"
     ]
    }
   ],
   "source": [
    "qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(\n",
    "    weight_dtype = torch.quint4x2,  #torch.qint8\n",
    "    lowp_mode = ipex.quantization.WoqLowpMode.NONE, #FP16, BF16, INT8\n",
    ")\n",
    "checkpoint=None\n",
    "model_ipex=ipex.llm.optimize(model,quantization_config=qconfig,low_precision_checkpoint=checkpoint)\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ebf49e9-3cfd-42f0-89cc-b8a1e5df9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message= \"\"\"\\n\\n You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "prompt= \"\\n\\n You are an expert in astronomy. Can you tell me 5 fun facts about the universe?\"\n",
    "model_answer_1 = 'None'\n",
    "\n",
    "prompt_tempate = f\"\"\"\n",
    "### System:\n",
    "{system_message}\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt_tempate, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8383c466-c7ac-42a7-a4ac-ddecf9d5416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) The Universe is expanding at such speeds that if we could travel far enough into space (and time), it would be easier for us to get back home than when we left! This phenomenon was first predicted by Albert Einstein through his theory of general relativity. It has been confirmed with observations from cosmic microwave background radiation data.\n",
      "2) Our Milky Way Galaxy contains over one hundred billion stars - but this number keeps growing because new ones form all the time due to star births within interstellar clouds or remnants after supernovas have exploded. We can only see around two thousand galaxies without using telescopes; however, there may exist more like ten times our own galaxy-counting estimate. So, imagine how many celestial bodies might actually reside out beyond what human eyes perceive now.\n",
      "3) Black holes aren’t just found deep inside massive galactic cores – they also lurk much closer on smaller scales too! In fact, some scientists believe tiny black hole \"seeds\" were created during Big Bang nucleosynthesis which then grew larger via accretion processes later forming stellar systems including planets & life forms themselves…a truly mindboggling concept indeed!\n",
      "4) A single teaspoon full of water weighs approximately five pounds here on Earth—but try imagining its weightlessness under zero gravity conditions where no force pulls downwards anymore...it becomes infinitely lightweight relative to terrest\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer,skip_prompt=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    tokens = model_ipex.generate(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=300,\n",
    "        repetition_penalty=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71442ce8-05e0-4bbf-b679-457386859b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7af949-6e5a-48cd-9ce5-71303df7753f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
